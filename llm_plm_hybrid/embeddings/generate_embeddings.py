#!/usr/bin/env python3
import os
import warnings

import torch
import numpy as np
import pandas as pd
import umap          # pip install umap-learn
import matplotlib.pyplot as plt
from pathlib import Path
from tqdm import tqdm

print("ğŸ”„ Starting embedding generationâ€¦")

# â”€â”€ Device setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
USE_CUDA = torch.cuda.is_available()
print(f"âš™ï¸ CUDA available? {'Yes' if USE_CUDA else 'No'}")
DEVICE = "cuda" if USE_CUDA else "cpu"

# â”€â”€ Load two independent ESM-2 instances â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("ğŸ“¦ Loading ESM-2 for GPUâ€¦")
esm_gpu, alphabet = torch.hub.load(
    "facebookresearch/esm:main", "esm2_t6_8M_UR50D"
)
esm_gpu = esm_gpu.eval().to("cuda")  # separate copy for GPU
print("ğŸ“¦ Loading ESM-2 for CPUâ€¦")
esm_cpu, _ = torch.hub.load(
    "facebookresearch/esm:main", "esm2_t6_8M_UR50D"
)
esm_cpu = esm_cpu.eval().to("cpu")   # separate copy for CPU
batch_converter = alphabet.get_batch_converter()
print("âœ… ESM-2 ready on both devices")

# â”€â”€ Chunking parameters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MAX_SEQ_LEN = 2500   # large max to handle long seqs
CHUNK_STEP  = 1022
REPORT_EVERY = 1000

# â”€â”€ Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DATA_DIR = Path(__file__).resolve().parent.parent / "data"
EMB_DIR  = Path(__file__).resolve().parent
VIS_DIR  = EMB_DIR.parent / "visualizations"

os.makedirs(EMB_DIR, exist_ok=True)
os.makedirs(VIS_DIR, exist_ok=True)

def embed_sequence(seq: str) -> np.ndarray:
    """
    Embed an arbitrarily long protein by slicing into â‰¤CHUNK_STEP windows,
    running each window on GPU first (then CPU on OOM), and averaging.
    """
    windows = [seq[i:i + MAX_SEQ_LEN] for i in range(0, len(seq), CHUNK_STEP)]
    embs = []

    for w in windows:
        _, _, toks = batch_converter([("id", w)])
        for dev, model in (("cuda", esm_gpu), ("cpu", esm_cpu)):
            if dev == "cuda" and not USE_CUDA:
                continue
            try:
                toks_dev = toks.to(dev)
                with torch.no_grad():
                    reps = model(toks_dev, repr_layers=[6])["representations"][6]
                emb = reps.mean(1).cpu().numpy()
                embs.append(emb)
                break
            except RuntimeError as e:
                if dev == "cuda" and "out of memory" in str(e).lower():
                    torch.cuda.empty_cache()
                    print(f"âš ï¸ GPU OOM on window len {len(w)}, retrying CPUâ€¦")
                    continue
                raise

    return np.mean(embs, axis=0)

def process_split(split_name: str):
    """
    Read data/<split_name>.csv, embed sequences, and save:
      embeddings/<split_name>.npz (with X, y, meta arrays)
    """
    csv_path = DATA_DIR / f"{split_name}.csv"
    print(f"\nğŸ“¥ Loading {csv_path}â€¦")
    df = pd.read_csv(csv_path)
    seqs = df["sequence"].tolist()
    accessions = df["accession"].tolist()
    labels = df.iloc[:, 2].values

    all_embs = []
    for i, seq in enumerate(tqdm(seqs, total=len(seqs)), start=1):
        emb = embed_sequence(seq)
        all_embs.append(emb)
        if i % REPORT_EVERY == 0 or i == len(seqs):
            print(f"   â†³ Completed {i}/{len(seqs)} embeddings")

    X    = np.stack(all_embs)
    y    = labels
    meta = np.array(accessions, dtype=object)

    out_path = EMB_DIR / f"{split_name}.npz"
    np.savez_compressed(out_path, X=X, y=y, meta=meta)
    print(f"âœ… Saved embeddings+meta â†’ {out_path} (shape={X.shape})")

if __name__ == "__main__":
    # â”€â”€ Process all splits â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    splits = [
        "classification_train", "classification_val", "classification_test",
        "regression_train",     "regression_val",     "regression_test"
    ]
    for sp in splits:
        process_split(sp)

    print("\nğŸ‰ All embeddings generated!")

    # â”€â”€ Labeled UMAP on train split â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("ğŸ—ºï¸ Computing UMAP on classification_train embeddingsâ€¦")
    train_npz = EMB_DIR / "classification_train.npz"
    data = np.load(train_npz, allow_pickle=True)
    X = np.squeeze(data["X"])
    y = data["y"]

    reducer = umap.UMAP(n_components=2, random_state=42)
    Z = reducer.fit_transform(X)

    plt.figure(figsize=(8,8))
    scatter = plt.scatter(Z[:,0], Z[:,1], c=y, s=5, alpha=0.7)
    plt.xlabel("UMAP Dimension 1")
    plt.ylabel("UMAP Dimension 2")
    plt.title("UMAP of ESM-2 Embeddings\nColored by Existence Level")
    cbar = plt.colorbar(scatter)
    cbar.set_label("Existence Level")

    umap_path = VIS_DIR / "train_umap_labeled.png"
    plt.savefig(umap_path)
    print(f"ğŸ“Š Saved labeled UMAP plot â†’ {umap_path}")
    plt.close()
