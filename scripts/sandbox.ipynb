{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "# adjust to point at your repoâ€™s scripts folder\n",
    "# REPO_ROOT = os.path.abspath(\"/home/hice1/snachimuthu7/CSE7850/class_project/LLM-PLM-Hybrid-Project\")\n",
    "# SCRIPTS   = os.path.join(REPO_ROOT, \"scripts\")\n",
    "\n",
    "# if SCRIPTS not in sys.path:\n",
    "#     sys.path.insert(0, SCRIPTS)\n",
    "\n",
    "# print(\"scripts folder:\", SCRIPTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Attempting STREAM endpointâ€¦\n",
      "âŒ STREAM failed (403 Client Error:  for url: https://rest.uniprot.org/uniprotkb/stream?query=%2A&format=tsv&fields=accession%2Csequence%2Cprotein_existence%2Cft_mod_res&compressed=true), falling back to JSONâ€¦\n",
      "ğŸ“„ Page 1: 500 entries\n",
      "ğŸ“„ Page 2: 500 entries\n",
      "ğŸ“„ Page 3: 500 entries\n",
      "ğŸ“„ Page 4: 500 entries\n",
      "ğŸ“„ Page 5: 500 entries\n",
      "ğŸ“„ Page 6: 500 entries\n",
      "ğŸ“„ Page 7: 500 entries\n",
      "ğŸ“„ Page 8: 500 entries\n",
      "ğŸ“„ Page 9: 500 entries\n",
      "ğŸ“„ Page 10: 500 entries\n",
      "ğŸ“„ Page 11: 500 entries\n",
      "ğŸ“„ Page 12: 500 entries\n",
      "ğŸ“„ Page 13: 500 entries\n",
      "ğŸ“„ Page 14: 500 entries\n",
      "ğŸ“„ Page 15: 500 entries\n",
      "ğŸ“„ Page 16: 500 entries\n",
      "ğŸ“„ Page 17: 500 entries\n",
      "ğŸ“„ Page 18: 500 entries\n",
      "ğŸ“„ Page 19: 500 entries\n",
      "ğŸ“„ Page 20: 500 entries\n",
      "ğŸ“„ Page 21: 500 entries\n",
      "ğŸ“„ Page 22: 500 entries\n",
      "ğŸ“„ Page 23: 500 entries\n",
      "ğŸ“„ Page 24: 500 entries\n",
      "ğŸ“„ Page 25: 500 entries\n",
      "ğŸ“„ Page 26: 500 entries\n",
      "ğŸ“„ Page 27: 500 entries\n",
      "ğŸ“„ Page 28: 500 entries\n",
      "ğŸ“„ Page 29: 500 entries\n",
      "ğŸ“„ Page 30: 500 entries\n",
      "ğŸ“„ Page 31: 500 entries\n",
      "ğŸ“„ Page 32: 500 entries\n",
      "ğŸ“„ Page 33: 500 entries\n",
      "ğŸ“„ Page 34: 500 entries\n",
      "ğŸ“„ Page 35: 500 entries\n",
      "ğŸ“„ Page 36: 500 entries\n",
      "ğŸ“„ Page 37: 500 entries\n",
      "ğŸ“„ Page 38: 500 entries\n",
      "ğŸ“„ Page 39: 500 entries\n",
      "ğŸ“„ Page 40: 500 entries\n",
      "ğŸ“„ Page 41: 500 entries\n",
      "ğŸ“„ Page 42: 500 entries\n",
      "ğŸ“„ Page 43: 500 entries\n",
      "ğŸ“„ Page 44: 500 entries\n",
      "ğŸ“„ Page 45: 500 entries\n",
      "ğŸ“„ Page 46: 500 entries\n",
      "ğŸ“„ Page 47: 500 entries\n",
      "ğŸ“„ Page 48: 500 entries\n",
      "ğŸ“„ Page 49: 500 entries\n",
      "ğŸ“„ Page 50: 500 entries\n",
      "ğŸ“„ Page 51: 500 entries\n",
      "ğŸ“„ Page 52: 500 entries\n",
      "ğŸ“„ Page 53: 500 entries\n",
      "ğŸ“„ Page 54: 500 entries\n",
      "ğŸ“„ Page 55: 500 entries\n",
      "ğŸ“„ Page 56: 500 entries\n",
      "ğŸ“„ Page 57: 500 entries\n",
      "ğŸ“„ Page 58: 500 entries\n",
      "ğŸ“„ Page 59: 500 entries\n",
      "ğŸ“„ Page 60: 500 entries\n",
      "ğŸ“„ Page 61: 500 entries\n",
      "ğŸ“„ Page 62: 500 entries\n",
      "ğŸ“„ Page 63: 500 entries\n",
      "ğŸ“„ Page 64: 500 entries\n",
      "ğŸ“„ Page 65: 500 entries\n",
      "ğŸ“„ Page 66: 500 entries\n",
      "ğŸ“„ Page 67: 500 entries\n",
      "ğŸ“„ Page 68: 500 entries\n",
      "ğŸ“„ Page 69: 500 entries\n",
      "ğŸ“„ Page 70: 500 entries\n",
      "ğŸ“„ Page 71: 500 entries\n",
      "ğŸ“„ Page 72: 500 entries\n",
      "ğŸ“„ Page 73: 500 entries\n",
      "ğŸ“„ Page 74: 500 entries\n",
      "ğŸ“„ Page 75: 500 entries\n",
      "ğŸ“„ Page 76: 500 entries\n",
      "ğŸ“„ Page 77: 500 entries\n",
      "ğŸ“„ Page 78: 500 entries\n",
      "ğŸ“„ Page 79: 500 entries\n",
      "ğŸ“„ Page 80: 500 entries\n",
      "ğŸ“„ Page 81: 500 entries\n",
      "ğŸ“„ Page 82: 500 entries\n",
      "ğŸ“„ Page 83: 500 entries\n",
      "ğŸ“„ Page 84: 500 entries\n",
      "ğŸ“„ Page 85: 500 entries\n",
      "ğŸ“„ Page 86: 500 entries\n",
      "ğŸ“„ Page 87: 500 entries\n",
      "ğŸ“„ Page 88: 500 entries\n",
      "ğŸ“„ Page 89: 500 entries\n",
      "ğŸ“„ Page 90: 500 entries\n",
      "ğŸ“„ Page 91: 500 entries\n",
      "ğŸ“„ Page 92: 500 entries\n",
      "ğŸ“„ Page 93: 500 entries\n",
      "ğŸ“„ Page 94: 500 entries\n",
      "ğŸ“„ Page 95: 500 entries\n",
      "ğŸ“„ Page 96: 500 entries\n",
      "ğŸ“„ Page 97: 500 entries\n",
      "ğŸ“„ Page 98: 500 entries\n",
      "ğŸ“„ Page 99: 500 entries\n",
      "ğŸ“„ Page 100: 500 entries\n",
      "âœ… Finished JSON search â€” pages fetched: 100\n",
      "âœ… JSON fallback succeeded: wrote 50000 records.\n"
     ]
    }
   ],
   "source": [
    "# 2.1 Create classification.csv and regression.csv\n",
    "from create_corpus import main as create_corpus\n",
    "create_corpus()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ”ï¸ classification.csv: 35000/7500/7500 train/val/test rows\n",
      "âœ”ï¸ regression.csv: 35000/7500/7500 train/val/test rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import split_corpus  # moduleâ€™s topâ€level code will execute splitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Starting embedding generationâ€¦\n",
      "ğŸ“¦ Loading ESM-2 modelâ€¦\n",
      "âœ… ESM-2 ready on cuda\n",
      "ğŸ“¥ Loading classification_train.csvâ€¦\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 16772/35000 [02:25<02:34, 118.13it/s]/home/hice1/snachimuthu7/CSE7850/class_project/LLM-PLM-Hybrid-Project/scripts/generate_embeddings.py:41: UserWarning: ğŸ”¥ OOM on GPU, retrying on CPU\n",
      "  warnings.warn(\"ğŸ”¥ OOM on GPU, retrying on CPU\")\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35000/35000 [06:03<00:00, 96.17it/s]\n",
      "âœ… Saved embeddings for classification_train\n",
      "ğŸ“¥ Loading classification_val.csvâ€¦\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7500/7500 [01:04<00:00, 116.03it/s]\n",
      "âœ… Saved embeddings for classification_val\n",
      "ğŸ“¥ Loading classification_test.csvâ€¦\n",
      "  5%|â–ˆâ–Š                                     | 357/7500 [00:03<01:02, 114.19it/s]/home/hice1/snachimuthu7/CSE7850/class_project/LLM-PLM-Hybrid-Project/scripts/generate_embeddings.py:41: UserWarning: ğŸ”¥ OOM on GPU, retrying on CPU\n",
      "  warnings.warn(\"ğŸ”¥ OOM on GPU, retrying on CPU\")\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7500/7500 [02:08<00:00, 58.59it/s]\n",
      "âœ… Saved embeddings for classification_test\n",
      "ğŸ“¥ Loading regression_train.csvâ€¦\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 16778/35000 [02:20<02:44, 110.91it/s]/home/hice1/snachimuthu7/CSE7850/class_project/LLM-PLM-Hybrid-Project/scripts/generate_embeddings.py:41: UserWarning: ğŸ”¥ OOM on GPU, retrying on CPU\n",
      "  warnings.warn(\"ğŸ”¥ OOM on GPU, retrying on CPU\")\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35000/35000 [05:59<00:00, 97.41it/s]\n",
      "âœ… Saved embeddings for regression_train\n",
      "ğŸ“¥ Loading regression_val.csvâ€¦\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7500/7500 [01:04<00:00, 115.94it/s]\n",
      "âœ… Saved embeddings for regression_val\n",
      "ğŸ“¥ Loading regression_test.csvâ€¦\n",
      "  5%|â–ˆâ–Š                                     | 360/7500 [00:03<01:03, 111.83it/s]/home/hice1/snachimuthu7/CSE7850/class_project/LLM-PLM-Hybrid-Project/scripts/generate_embeddings.py:41: UserWarning: ğŸ”¥ OOM on GPU, retrying on CPU\n",
      "  warnings.warn(\"ğŸ”¥ OOM on GPU, retrying on CPU\")\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7500/7500 [02:07<00:00, 58.68it/s]\n",
      "âœ… Saved embeddings for regression_test\n",
      "ğŸ‰ All embeddings generated!\n"
     ]
    }
   ],
   "source": [
    "# from generate_embeddings import process_split\n",
    "\n",
    "# splits = [\n",
    "#     \"classification_train\", \"classification_val\", \"classification_test\",\n",
    "#     \"regression_train\",     \"regression_val\",     \"regression_test\"\n",
    "# ]\n",
    "\n",
    "# for s in splits:\n",
    "#     print(f\"Embedding split: {s}\")\n",
    "#     process_split(s)\n",
    "!python generate_embeddings.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Starting tiny-heads trainingâ€¦\n",
      "ğŸ“¥ Loading embeddings/classification_train.npzâ€¦\n",
      "ğŸ“¥ Loading embeddings/classification_val.npzâ€¦\n",
      "â„¹ï¸ emb dim: 320\n",
      "ğŸ·ï¸ Epoch 01: train=1.5911, val=1.5130\n",
      "âœ… Saved best pe head (loss=1.5130)\n",
      "ğŸ·ï¸ Epoch 02: train=1.5132, val=1.4379\n",
      "âœ… Saved best pe head (loss=1.4379)\n",
      "ğŸ·ï¸ Epoch 03: train=1.4374, val=1.3652\n",
      "âœ… Saved best pe head (loss=1.3652)\n",
      "ğŸ·ï¸ Epoch 04: train=1.3641, val=1.2946\n",
      "âœ… Saved best pe head (loss=1.2946)\n",
      "ğŸ·ï¸ Epoch 05: train=1.2930, val=1.2256\n",
      "âœ… Saved best pe head (loss=1.2256)\n",
      "ğŸ·ï¸ Epoch 06: train=1.2235, val=1.1577\n",
      "âœ… Saved best pe head (loss=1.1577)\n",
      "ğŸ·ï¸ Epoch 07: train=1.1550, val=1.0905\n",
      "âœ… Saved best pe head (loss=1.0905)\n",
      "ğŸ·ï¸ Epoch 08: train=1.0873, val=1.0241\n",
      "âœ… Saved best pe head (loss=1.0241)\n",
      "ğŸ·ï¸ Epoch 09: train=1.0204, val=0.9588\n",
      "âœ… Saved best pe head (loss=0.9588)\n",
      "ğŸ·ï¸ Epoch 10: train=0.9546, val=0.8952\n",
      "âœ… Saved best pe head (loss=0.8952)\n",
      "ğŸ‰ Head training complete!\n",
      "ğŸ“¥ Loading embeddings/regression_train.npzâ€¦\n",
      "ğŸ“¥ Loading embeddings/regression_val.npzâ€¦\n",
      "ğŸ·ï¸ Epoch 01: train=0.5210, val=0.4662\n",
      "âœ… Saved best ptm head (loss=0.4662)\n",
      "ğŸ·ï¸ Epoch 02: train=0.4780, val=0.4331\n",
      "âœ… Saved best ptm head (loss=0.4331)\n",
      "ğŸ·ï¸ Epoch 03: train=0.4439, val=0.4090\n",
      "âœ… Saved best ptm head (loss=0.4090)\n",
      "ğŸ·ï¸ Epoch 04: train=0.4189, val=0.3935\n",
      "âœ… Saved best ptm head (loss=0.3935)\n",
      "ğŸ·ï¸ Epoch 05: train=0.4025, val=0.3859\n",
      "âœ… Saved best ptm head (loss=0.3859)\n",
      "ğŸ·ï¸ Epoch 06: train=0.3940, val=0.3846\n",
      "âœ… Saved best ptm head (loss=0.3846)\n",
      "ğŸ·ï¸ Epoch 07: train=0.3918, val=0.3870\n",
      "ğŸ·ï¸ Epoch 08: train=0.3935, val=0.3896\n",
      "ğŸ·ï¸ Epoch 09: train=0.3954, val=0.3899\n",
      "ğŸ·ï¸ Epoch 10: train=0.3952, val=0.3875\n",
      "ğŸ‰ Head training complete!\n"
     ]
    }
   ],
   "source": [
    "# This will write pe.pt and ptm.pt under tiny_heads/\n",
    "!python train_heads.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Loading embeddings/classification_train.npzâ€¦\n",
      "â„¹ï¸ Squeezing singleton dimension: (35000, 1, 320) â†’ (35000, 320)\n",
      "â„¹ï¸ Loaded (35000, 320) (n_vectors, dim)\n",
      "âš™ï¸ Normalizing & building indexâ€¦\n",
      "âœ… Index saved to embeddings/classification_train.index\n"
     ]
    }
   ],
   "source": [
    "# from faiss_index import build_faiss_index\n",
    "# build_faiss_index(\n",
    "#     emb_file=\"embeddings/classification_train.npz\",\n",
    "#     idx_file=\"embeddings/classification_train.index\"\n",
    "# )\n",
    "\n",
    "!python faiss_index.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745593552.483861 1409461 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745593552.488851 1409461 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "ğŸ“¦ Loading intent model (BioBERT)â€¦\n",
      "âœ… BioBERT intent model ready\n",
      "ğŸ“¦ Loading ESM-2 embedderâ€¦\n",
      "âœ… ESM-2 ready on cuda\n",
      "ğŸ“¦ Loading tiny_heads modelsâ€¦\n",
      "âœ… tiny_heads loaded\n",
      "â–¶ What is the protein existence level of lactase?\n",
      "â„¹ï¸ raw : What is the protein existence level of lactase?\n",
      "â„¹ï¸ norm: what is the protein existence level of lactase\n",
      "   â†’ task=protein_existence, acc=None, raw=no, name=lactase, org=None\n",
      "ğŸ” Strict search 'lactase'â€¦\n",
      "âœ… Found None for 'lactase'\n",
      "â†’ âš ï¸ No reviewed UniProt accession found for 'lactase'. \n",
      "\n",
      "â–¶ How many predicted PTM sites are there for P09812?\n",
      "â„¹ï¸ raw : How many predicted PTM sites are there for P09812?\n",
      "â„¹ï¸ norm: how many predicted ptm sites are there for p09812\n",
      "   â†’ task=ptm_count, acc=P09812, raw=no, name=None, org=None\n",
      "ğŸ”„ Fetching UniProt entry for P09812â€¦\n",
      "âœ… UniProt data: len=842, pe=1: Evidence at protein level, ptm=11\n",
      "â†’ ğŸ“– UniProt lists 11 PTM sites. \n",
      "\n",
      "â–¶ PTM count for MVHFAELVK?\n",
      "â„¹ï¸ raw : PTM count for MVHFAELVK?\n",
      "â„¹ï¸ norm: ptm count for mvhfaelvk\n",
      "   â†’ task=ptm_count, acc=None, raw=yes, name=None, org=None\n",
      "â†’ ğŸ¤– I predict ~0.9 PTM sites. \n",
      "\n",
      "â–¶ Level of protein existence for ACDEFGHIKL?\n",
      "â„¹ï¸ raw : Level of protein existence for ACDEFGHIKL?\n",
      "â„¹ï¸ norm: level of protein existence for acdefghikl\n",
      "   â†’ task=protein_existence, acc=None, raw=yes, name=None, org=None\n",
      "â†’ ğŸ¤– I predict level 1 (evidence at protein level). \n",
      "\n",
      "â–¶ What is the existence level of mouse hemoglobin?\n",
      "â„¹ï¸ raw : What is the existence level of mouse hemoglobin?\n",
      "â„¹ï¸ norm: what is the existence level of mouse hemoglobin\n",
      "   â†’ task=protein_existence, acc=None, raw=no, name=hemoglobin, org=10090\n",
      "ğŸ” Strict search 'hemoglobin'â€¦\n",
      "âœ… Found None for 'hemoglobin'\n",
      "â†’ âš ï¸ No reviewed UniProt accession found for 'hemoglobin'. \n",
      "\n",
      "â–¶ PTM count for E. coli DnaK?\n",
      "â„¹ï¸ raw : PTM count for E. coli DnaK?\n",
      "â„¹ï¸ norm: ptm count for e coli dnak\n",
      "   â†’ task=ptm_count, acc=None, raw=no, name=dnak, org=562\n",
      "ğŸ” Strict search 'dnak'â€¦\n",
      "/home/hice1/snachimuthu7/CSE7850/class_project/LLM-PLM-Hybrid-Project/scripts/retrieval.py:37: UserWarning: âš ï¸ Strict failed for 'dnak', retryingâ€¦\n",
      "  warnings.warn(f\"âš ï¸ Strict failed for '{name}', retryingâ€¦\")\n",
      "âœ… Found None for 'dnak'\n",
      "â†’ âš ï¸ No reviewed UniProt accession found for 'dnak'. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from rag_pipeline import answer\n",
    "\n",
    "# examples = [\n",
    "#     \"What is the protein existence level of lactase?\",\n",
    "#     \"How many predicted PTM sites are there for P09812?\",\n",
    "#     \"PTM count for MVHFAELVK?\",\n",
    "#     \"Level of protein existence for ACDEFGHIKL?\",\n",
    "#     \"What is the existence level of mouse hemoglobin?\",\n",
    "#     \"PTM count for E. coli DnaK?\"\n",
    "# ]\n",
    "\n",
    "# for q in examples:\n",
    "#     print(\"â–¶\", q)\n",
    "#     print(\"â†’\", answer(q), \"\\n\")\n",
    "\n",
    "!python rag_pipeline.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: uvicorn: command not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# In one cell you can launch the APIâ€”but note this will block the kernel:\n",
    "!uvicorn serve_api:app --reload --port 8000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
