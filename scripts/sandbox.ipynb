{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "# adjust to point at your repo’s scripts folder\n",
    "# REPO_ROOT = os.path.abspath(\"/home/hice1/snachimuthu7/CSE7850/class_project/LLM-PLM-Hybrid-Project\")\n",
    "# SCRIPTS   = os.path.join(REPO_ROOT, \"scripts\")\n",
    "\n",
    "# if SCRIPTS not in sys.path:\n",
    "#     sys.path.insert(0, SCRIPTS)\n",
    "\n",
    "# print(\"scripts folder:\", SCRIPTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Attempting STREAM endpoint…\n",
      "❌ STREAM failed (403 Client Error:  for url: https://rest.uniprot.org/uniprotkb/stream?query=%2A&format=tsv&fields=accession%2Csequence%2Cprotein_existence%2Cft_mod_res&compressed=true), falling back to JSON…\n",
      "📄 Page 1: 500 entries\n",
      "📄 Page 2: 500 entries\n",
      "📄 Page 3: 500 entries\n",
      "📄 Page 4: 500 entries\n",
      "📄 Page 5: 500 entries\n",
      "📄 Page 6: 500 entries\n",
      "📄 Page 7: 500 entries\n",
      "📄 Page 8: 500 entries\n",
      "📄 Page 9: 500 entries\n",
      "📄 Page 10: 500 entries\n",
      "📄 Page 11: 500 entries\n",
      "📄 Page 12: 500 entries\n",
      "📄 Page 13: 500 entries\n",
      "📄 Page 14: 500 entries\n",
      "📄 Page 15: 500 entries\n",
      "📄 Page 16: 500 entries\n",
      "📄 Page 17: 500 entries\n",
      "📄 Page 18: 500 entries\n",
      "📄 Page 19: 500 entries\n",
      "📄 Page 20: 500 entries\n",
      "📄 Page 21: 500 entries\n",
      "📄 Page 22: 500 entries\n",
      "📄 Page 23: 500 entries\n",
      "📄 Page 24: 500 entries\n",
      "📄 Page 25: 500 entries\n",
      "📄 Page 26: 500 entries\n",
      "📄 Page 27: 500 entries\n",
      "📄 Page 28: 500 entries\n",
      "📄 Page 29: 500 entries\n",
      "📄 Page 30: 500 entries\n",
      "📄 Page 31: 500 entries\n",
      "📄 Page 32: 500 entries\n",
      "📄 Page 33: 500 entries\n",
      "📄 Page 34: 500 entries\n",
      "📄 Page 35: 500 entries\n",
      "📄 Page 36: 500 entries\n",
      "📄 Page 37: 500 entries\n",
      "📄 Page 38: 500 entries\n",
      "📄 Page 39: 500 entries\n",
      "📄 Page 40: 500 entries\n",
      "📄 Page 41: 500 entries\n",
      "📄 Page 42: 500 entries\n",
      "📄 Page 43: 500 entries\n",
      "📄 Page 44: 500 entries\n",
      "📄 Page 45: 500 entries\n",
      "📄 Page 46: 500 entries\n",
      "📄 Page 47: 500 entries\n",
      "📄 Page 48: 500 entries\n",
      "📄 Page 49: 500 entries\n",
      "📄 Page 50: 500 entries\n",
      "📄 Page 51: 500 entries\n",
      "📄 Page 52: 500 entries\n",
      "📄 Page 53: 500 entries\n",
      "📄 Page 54: 500 entries\n",
      "📄 Page 55: 500 entries\n",
      "📄 Page 56: 500 entries\n",
      "📄 Page 57: 500 entries\n",
      "📄 Page 58: 500 entries\n",
      "📄 Page 59: 500 entries\n",
      "📄 Page 60: 500 entries\n",
      "📄 Page 61: 500 entries\n",
      "📄 Page 62: 500 entries\n",
      "📄 Page 63: 500 entries\n",
      "📄 Page 64: 500 entries\n",
      "📄 Page 65: 500 entries\n",
      "📄 Page 66: 500 entries\n",
      "📄 Page 67: 500 entries\n",
      "📄 Page 68: 500 entries\n",
      "📄 Page 69: 500 entries\n",
      "📄 Page 70: 500 entries\n",
      "📄 Page 71: 500 entries\n",
      "📄 Page 72: 500 entries\n",
      "📄 Page 73: 500 entries\n",
      "📄 Page 74: 500 entries\n",
      "📄 Page 75: 500 entries\n",
      "📄 Page 76: 500 entries\n",
      "📄 Page 77: 500 entries\n",
      "📄 Page 78: 500 entries\n",
      "📄 Page 79: 500 entries\n",
      "📄 Page 80: 500 entries\n",
      "📄 Page 81: 500 entries\n",
      "📄 Page 82: 500 entries\n",
      "📄 Page 83: 500 entries\n",
      "📄 Page 84: 500 entries\n",
      "📄 Page 85: 500 entries\n",
      "📄 Page 86: 500 entries\n",
      "📄 Page 87: 500 entries\n",
      "📄 Page 88: 500 entries\n",
      "📄 Page 89: 500 entries\n",
      "📄 Page 90: 500 entries\n",
      "📄 Page 91: 500 entries\n",
      "📄 Page 92: 500 entries\n",
      "📄 Page 93: 500 entries\n",
      "📄 Page 94: 500 entries\n",
      "📄 Page 95: 500 entries\n",
      "📄 Page 96: 500 entries\n",
      "📄 Page 97: 500 entries\n",
      "📄 Page 98: 500 entries\n",
      "📄 Page 99: 500 entries\n",
      "📄 Page 100: 500 entries\n",
      "✅ Finished JSON search — pages fetched: 100\n",
      "✅ JSON fallback succeeded: wrote 50000 records.\n"
     ]
    }
   ],
   "source": [
    "# 2.1 Create classification.csv and regression.csv\n",
    "from create_corpus import main as create_corpus\n",
    "create_corpus()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ classification.csv: 35000/7500/7500 train/val/test rows\n",
      "✔️ regression.csv: 35000/7500/7500 train/val/test rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import split_corpus  # module’s top‐level code will execute splitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Starting embedding generation…\n",
      "📦 Loading ESM-2 model…\n",
      "✅ ESM-2 ready on cuda\n",
      "📥 Loading classification_train.csv…\n",
      " 48%|█████████████████▎                  | 16772/35000 [02:25<02:34, 118.13it/s]/home/hice1/snachimuthu7/CSE7850/class_project/LLM-PLM-Hybrid-Project/scripts/generate_embeddings.py:41: UserWarning: 🔥 OOM on GPU, retrying on CPU\n",
      "  warnings.warn(\"🔥 OOM on GPU, retrying on CPU\")\n",
      "100%|█████████████████████████████████████| 35000/35000 [06:03<00:00, 96.17it/s]\n",
      "✅ Saved embeddings for classification_train\n",
      "📥 Loading classification_val.csv…\n",
      "100%|██████████████████████████████████████| 7500/7500 [01:04<00:00, 116.03it/s]\n",
      "✅ Saved embeddings for classification_val\n",
      "📥 Loading classification_test.csv…\n",
      "  5%|█▊                                     | 357/7500 [00:03<01:02, 114.19it/s]/home/hice1/snachimuthu7/CSE7850/class_project/LLM-PLM-Hybrid-Project/scripts/generate_embeddings.py:41: UserWarning: 🔥 OOM on GPU, retrying on CPU\n",
      "  warnings.warn(\"🔥 OOM on GPU, retrying on CPU\")\n",
      "100%|███████████████████████████████████████| 7500/7500 [02:08<00:00, 58.59it/s]\n",
      "✅ Saved embeddings for classification_test\n",
      "📥 Loading regression_train.csv…\n",
      " 48%|█████████████████▎                  | 16778/35000 [02:20<02:44, 110.91it/s]/home/hice1/snachimuthu7/CSE7850/class_project/LLM-PLM-Hybrid-Project/scripts/generate_embeddings.py:41: UserWarning: 🔥 OOM on GPU, retrying on CPU\n",
      "  warnings.warn(\"🔥 OOM on GPU, retrying on CPU\")\n",
      "100%|█████████████████████████████████████| 35000/35000 [05:59<00:00, 97.41it/s]\n",
      "✅ Saved embeddings for regression_train\n",
      "📥 Loading regression_val.csv…\n",
      "100%|██████████████████████████████████████| 7500/7500 [01:04<00:00, 115.94it/s]\n",
      "✅ Saved embeddings for regression_val\n",
      "📥 Loading regression_test.csv…\n",
      "  5%|█▊                                     | 360/7500 [00:03<01:03, 111.83it/s]/home/hice1/snachimuthu7/CSE7850/class_project/LLM-PLM-Hybrid-Project/scripts/generate_embeddings.py:41: UserWarning: 🔥 OOM on GPU, retrying on CPU\n",
      "  warnings.warn(\"🔥 OOM on GPU, retrying on CPU\")\n",
      "100%|███████████████████████████████████████| 7500/7500 [02:07<00:00, 58.68it/s]\n",
      "✅ Saved embeddings for regression_test\n",
      "🎉 All embeddings generated!\n"
     ]
    }
   ],
   "source": [
    "# from generate_embeddings import process_split\n",
    "\n",
    "# splits = [\n",
    "#     \"classification_train\", \"classification_val\", \"classification_test\",\n",
    "#     \"regression_train\",     \"regression_val\",     \"regression_test\"\n",
    "# ]\n",
    "\n",
    "# for s in splits:\n",
    "#     print(f\"Embedding split: {s}\")\n",
    "#     process_split(s)\n",
    "!python generate_embeddings.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Starting tiny-heads training…\n",
      "📥 Loading embeddings/classification_train.npz…\n",
      "📥 Loading embeddings/classification_val.npz…\n",
      "ℹ️ emb dim: 320\n",
      "🏷️ Epoch 01: train=1.5911, val=1.5130\n",
      "✅ Saved best pe head (loss=1.5130)\n",
      "🏷️ Epoch 02: train=1.5132, val=1.4379\n",
      "✅ Saved best pe head (loss=1.4379)\n",
      "🏷️ Epoch 03: train=1.4374, val=1.3652\n",
      "✅ Saved best pe head (loss=1.3652)\n",
      "🏷️ Epoch 04: train=1.3641, val=1.2946\n",
      "✅ Saved best pe head (loss=1.2946)\n",
      "🏷️ Epoch 05: train=1.2930, val=1.2256\n",
      "✅ Saved best pe head (loss=1.2256)\n",
      "🏷️ Epoch 06: train=1.2235, val=1.1577\n",
      "✅ Saved best pe head (loss=1.1577)\n",
      "🏷️ Epoch 07: train=1.1550, val=1.0905\n",
      "✅ Saved best pe head (loss=1.0905)\n",
      "🏷️ Epoch 08: train=1.0873, val=1.0241\n",
      "✅ Saved best pe head (loss=1.0241)\n",
      "🏷️ Epoch 09: train=1.0204, val=0.9588\n",
      "✅ Saved best pe head (loss=0.9588)\n",
      "🏷️ Epoch 10: train=0.9546, val=0.8952\n",
      "✅ Saved best pe head (loss=0.8952)\n",
      "🎉 Head training complete!\n",
      "📥 Loading embeddings/regression_train.npz…\n",
      "📥 Loading embeddings/regression_val.npz…\n",
      "🏷️ Epoch 01: train=0.5210, val=0.4662\n",
      "✅ Saved best ptm head (loss=0.4662)\n",
      "🏷️ Epoch 02: train=0.4780, val=0.4331\n",
      "✅ Saved best ptm head (loss=0.4331)\n",
      "🏷️ Epoch 03: train=0.4439, val=0.4090\n",
      "✅ Saved best ptm head (loss=0.4090)\n",
      "🏷️ Epoch 04: train=0.4189, val=0.3935\n",
      "✅ Saved best ptm head (loss=0.3935)\n",
      "🏷️ Epoch 05: train=0.4025, val=0.3859\n",
      "✅ Saved best ptm head (loss=0.3859)\n",
      "🏷️ Epoch 06: train=0.3940, val=0.3846\n",
      "✅ Saved best ptm head (loss=0.3846)\n",
      "🏷️ Epoch 07: train=0.3918, val=0.3870\n",
      "🏷️ Epoch 08: train=0.3935, val=0.3896\n",
      "🏷️ Epoch 09: train=0.3954, val=0.3899\n",
      "🏷️ Epoch 10: train=0.3952, val=0.3875\n",
      "🎉 Head training complete!\n"
     ]
    }
   ],
   "source": [
    "# This will write pe.pt and ptm.pt under tiny_heads/\n",
    "!python train_heads.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading embeddings/classification_train.npz…\n",
      "ℹ️ Squeezing singleton dimension: (35000, 1, 320) → (35000, 320)\n",
      "ℹ️ Loaded (35000, 320) (n_vectors, dim)\n",
      "⚙️ Normalizing & building index…\n",
      "✅ Index saved to embeddings/classification_train.index\n"
     ]
    }
   ],
   "source": [
    "# from faiss_index import build_faiss_index\n",
    "# build_faiss_index(\n",
    "#     emb_file=\"embeddings/classification_train.npz\",\n",
    "#     idx_file=\"embeddings/classification_train.index\"\n",
    "# )\n",
    "\n",
    "!python faiss_index.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745593552.483861 1409461 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745593552.488851 1409461 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "📦 Loading intent model (BioBERT)…\n",
      "✅ BioBERT intent model ready\n",
      "📦 Loading ESM-2 embedder…\n",
      "✅ ESM-2 ready on cuda\n",
      "📦 Loading tiny_heads models…\n",
      "✅ tiny_heads loaded\n",
      "▶ What is the protein existence level of lactase?\n",
      "ℹ️ raw : What is the protein existence level of lactase?\n",
      "ℹ️ norm: what is the protein existence level of lactase\n",
      "   → task=protein_existence, acc=None, raw=no, name=lactase, org=None\n",
      "🔍 Strict search 'lactase'…\n",
      "✅ Found None for 'lactase'\n",
      "→ ⚠️ No reviewed UniProt accession found for 'lactase'. \n",
      "\n",
      "▶ How many predicted PTM sites are there for P09812?\n",
      "ℹ️ raw : How many predicted PTM sites are there for P09812?\n",
      "ℹ️ norm: how many predicted ptm sites are there for p09812\n",
      "   → task=ptm_count, acc=P09812, raw=no, name=None, org=None\n",
      "🔄 Fetching UniProt entry for P09812…\n",
      "✅ UniProt data: len=842, pe=1: Evidence at protein level, ptm=11\n",
      "→ 📖 UniProt lists 11 PTM sites. \n",
      "\n",
      "▶ PTM count for MVHFAELVK?\n",
      "ℹ️ raw : PTM count for MVHFAELVK?\n",
      "ℹ️ norm: ptm count for mvhfaelvk\n",
      "   → task=ptm_count, acc=None, raw=yes, name=None, org=None\n",
      "→ 🤖 I predict ~0.9 PTM sites. \n",
      "\n",
      "▶ Level of protein existence for ACDEFGHIKL?\n",
      "ℹ️ raw : Level of protein existence for ACDEFGHIKL?\n",
      "ℹ️ norm: level of protein existence for acdefghikl\n",
      "   → task=protein_existence, acc=None, raw=yes, name=None, org=None\n",
      "→ 🤖 I predict level 1 (evidence at protein level). \n",
      "\n",
      "▶ What is the existence level of mouse hemoglobin?\n",
      "ℹ️ raw : What is the existence level of mouse hemoglobin?\n",
      "ℹ️ norm: what is the existence level of mouse hemoglobin\n",
      "   → task=protein_existence, acc=None, raw=no, name=hemoglobin, org=10090\n",
      "🔍 Strict search 'hemoglobin'…\n",
      "✅ Found None for 'hemoglobin'\n",
      "→ ⚠️ No reviewed UniProt accession found for 'hemoglobin'. \n",
      "\n",
      "▶ PTM count for E. coli DnaK?\n",
      "ℹ️ raw : PTM count for E. coli DnaK?\n",
      "ℹ️ norm: ptm count for e coli dnak\n",
      "   → task=ptm_count, acc=None, raw=no, name=dnak, org=562\n",
      "🔍 Strict search 'dnak'…\n",
      "/home/hice1/snachimuthu7/CSE7850/class_project/LLM-PLM-Hybrid-Project/scripts/retrieval.py:37: UserWarning: ⚠️ Strict failed for 'dnak', retrying…\n",
      "  warnings.warn(f\"⚠️ Strict failed for '{name}', retrying…\")\n",
      "✅ Found None for 'dnak'\n",
      "→ ⚠️ No reviewed UniProt accession found for 'dnak'. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from rag_pipeline import answer\n",
    "\n",
    "# examples = [\n",
    "#     \"What is the protein existence level of lactase?\",\n",
    "#     \"How many predicted PTM sites are there for P09812?\",\n",
    "#     \"PTM count for MVHFAELVK?\",\n",
    "#     \"Level of protein existence for ACDEFGHIKL?\",\n",
    "#     \"What is the existence level of mouse hemoglobin?\",\n",
    "#     \"PTM count for E. coli DnaK?\"\n",
    "# ]\n",
    "\n",
    "# for q in examples:\n",
    "#     print(\"▶\", q)\n",
    "#     print(\"→\", answer(q), \"\\n\")\n",
    "\n",
    "!python rag_pipeline.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: uvicorn: command not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# In one cell you can launch the API—but note this will block the kernel:\n",
    "!uvicorn serve_api:app --reload --port 8000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
